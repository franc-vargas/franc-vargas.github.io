[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Francisco Vargas",
    "section": "",
    "text": "Pontificia Universidad Católica de Chile | Masters in Statistics | March 2023 - December 2024\nPontificia Universidad Católica de Chile | Diploma in Biostatistics | May 2022 - December 2022\nUniversidad Mayor | Veterinarian | March 2011 - December 2016"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Francisco Vargas",
    "section": "",
    "text": "Pontificia Universidad Católica de Chile | Masters in Statistics | March 2023 - December 2024\nPontificia Universidad Católica de Chile | Diploma in Biostatistics | May 2022 - December 2022\nUniversidad Mayor | Veterinarian | March 2011 - December 2016"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Francisco Vargas",
    "section": "Experience",
    "text": "Experience\nIQVIA | Data Management Analyst | June 2022 - Present\nVirbac Chile | Clinical Trials Assistant | May 2021 - May 2022\nMowi Chile S.A. | Saltwater Site Assistant | September 2019 - January 2021\nMulti X S.A. | Saltwater Site Assistant | June 2018 - September 2019\nAquaChile S.A. | Saltwater Site Assistant | March 2018 - June 2018"
  },
  {
    "objectID": "projects/Banco de Chile Consumer Stock/index.html",
    "href": "projects/Banco de Chile Consumer Stock/index.html",
    "title": "Forecasting: Banco de Chile’s Consumer Stock",
    "section": "",
    "text": "The normal consumer stock (NCS) of a bank represents the amount of money that regular consumers invest in consumer credit. In this particular case, it is measured in Chilean Pesos (CLP). In this particular case, monthly NCS values were downloaded from xxxxx time series data bank. We also scraped the Monthy Index of Economic Activity (IMACEC), and the Interest Rate of the Central Bank of Chile.\n\n\nWe can talk about stationarity in time series in two ways, stationarity of the mean and stationarity of the variance. Stationarity of the mean means that the values are around a mean \\(\\mu\\), or a fixed value, while variance means that the spread around that mean is even and there’s no periods where it increases or decreases. Many times this assumptions are not met, which means that we need to evaluate our time series a bit differently. We can assess the stationarity of the mean and variance of the first difference, and see if the mean stabilizes and then if the variance continues to be heteroskedastic, we can apply a transformation (sqrt, log, etc) to the original series and then evaluate the first difference.\nTo assess this, the quickest and simplest way to do is to look at a graph of the series and, as we can see here, this is a non-stationary time-series, both in variance and mean.\n\n\n\n\n\n\n\n\n\nIn this particular case, variance was stabilized via log-transformation, and the first difference was stationary in mean:\n\n\n\n\n\n\n\n\n\nWe can see that despite transformations and first-difference there are some breaks on our time series, which will influence the outcome of the classical SARIMA model.\n\n\n\nACF and PACF plots helps us decide the initial values for the SARIMA part of the model, while CCF helps us check the extent of the correlation between our covariates that we’ll use for forecasting.\n\n\n\n\n\n\n\n\n\nOur PACF and ACF plots suggest an SARIMA structure, we’ll consider an ARIMA (3,1,1) and a SARMA (3,0,1) and since this is monthly data we’ll consider quarterly seasonality.\nFor the SARIMAX model, same ARIMA and SARMA parts were considered, but our covariates included the ln of IMACEC and ln of the Interest Rate for Consumer credits, both lagged by 12 months. This decision came from noticing the CCF plot which showed relevant correlations 12 months in the past, as well as noticing that only the marginal effects (or interaction) between the two covariates was the most relevant part.\nSince the lagged product was the most relevant part, we modeled the product as the sum of logarithms.\n\n\n\nWe can see that while the SARIMA model maintains the proper shape, but the trend is off. In the case of the SARIMAX model, our covariates fix that trend through a linear regression.\n\n\n\n\n\n\nAnd our model evaluation, which considering we’re only using two covariates + time, is relatively good.\n\n\n\n\n\nModel\nRMSE\nMAE\nMPE\nMAPE\nMASE\nME\n\n\n\n\nSARIMA\n125657.04\n117211.93\n-2.4%\n2.4%\n1.98236\n-117211.93\n\n\nSARIMAX\n41968.47\n34211.05\n-0.5%\n0.7%\n0.57860\n-24576.02"
  },
  {
    "objectID": "projects/Banco de Chile Consumer Stock/index.html#stationarity",
    "href": "projects/Banco de Chile Consumer Stock/index.html#stationarity",
    "title": "Forecasting: Banco de Chile’s Consumer Stock",
    "section": "",
    "text": "We can talk about stationarity in time series in two ways, stationarity of the mean and stationarity of the variance. Stationarity of the mean means that the values are around a mean \\(\\mu\\), or a fixed value, while variance means that the spread around that mean is even and there’s no periods where it increases or decreases. Many times this assumptions are not met, which means that we need to evaluate our time series a bit differently. We can assess the stationarity of the mean and variance of the first difference, and see if the mean stabilizes and then if the variance continues to be heteroskedastic, we can apply a transformation (sqrt, log, etc) to the original series and then evaluate the first difference.\nTo assess this, the quickest and simplest way to do is to look at a graph of the series and, as we can see here, this is a non-stationary time-series, both in variance and mean.\n\n\n\n\n\n\n\n\n\nIn this particular case, variance was stabilized via log-transformation, and the first difference was stationary in mean:\n\n\n\n\n\n\n\n\n\nWe can see that despite transformations and first-difference there are some breaks on our time series, which will influence the outcome of the classical SARIMA model."
  },
  {
    "objectID": "projects/Banco de Chile Consumer Stock/index.html#auto-correlations-partial-auto-correlations-cross-correlations",
    "href": "projects/Banco de Chile Consumer Stock/index.html#auto-correlations-partial-auto-correlations-cross-correlations",
    "title": "Forecasting: Banco de Chile’s Consumer Stock",
    "section": "",
    "text": "ACF and PACF plots helps us decide the initial values for the SARIMA part of the model, while CCF helps us check the extent of the correlation between our covariates that we’ll use for forecasting.\n\n\n\n\n\n\n\n\n\nOur PACF and ACF plots suggest an SARIMA structure, we’ll consider an ARIMA (3,1,1) and a SARMA (3,0,1) and since this is monthly data we’ll consider quarterly seasonality.\nFor the SARIMAX model, same ARIMA and SARMA parts were considered, but our covariates included the ln of IMACEC and ln of the Interest Rate for Consumer credits, both lagged by 12 months. This decision came from noticing the CCF plot which showed relevant correlations 12 months in the past, as well as noticing that only the marginal effects (or interaction) between the two covariates was the most relevant part.\nSince the lagged product was the most relevant part, we modeled the product as the sum of logarithms."
  },
  {
    "objectID": "projects/Banco de Chile Consumer Stock/index.html#evaluation",
    "href": "projects/Banco de Chile Consumer Stock/index.html#evaluation",
    "title": "Forecasting: Banco de Chile’s Consumer Stock",
    "section": "",
    "text": "We can see that while the SARIMA model maintains the proper shape, but the trend is off. In the case of the SARIMAX model, our covariates fix that trend through a linear regression.\n\n\n\n\n\n\nAnd our model evaluation, which considering we’re only using two covariates + time, is relatively good.\n\n\n\n\n\nModel\nRMSE\nMAE\nMPE\nMAPE\nMASE\nME\n\n\n\n\nSARIMA\n125657.04\n117211.93\n-2.4%\n2.4%\n1.98236\n-117211.93\n\n\nSARIMAX\n41968.47\n34211.05\n-0.5%\n0.7%\n0.57860\n-24576.02"
  },
  {
    "objectID": "projects/Freq_Bayesian_Binomial/index.html",
    "href": "projects/Freq_Bayesian_Binomial/index.html",
    "title": "Bayesian vs Frequentist Intervals, Binomial comparison",
    "section": "",
    "text": "Classic frequentist statistics is the main type of statistics that we’re taught at school and on most undergraduate programs, and for good reason. Frequentist statistics assume the parameter of interest is fixed and base their results on long-run events or samples. When creating a confidence interval at 95%, we’re saying that 95% of the intervals produced contain the true mean.\nBayesian interpretation of probability on the other hand, assumes that the parameter itself is a random variable that comes from a probability distribution. The Bayesian statistician will have to assume a prior probability for the parameter, a likelihood function and that will give a posterior probability where we can calculate a posteriori Credible Interval for our parameter of interest.\nIt is often said that the bayesian approach to statistics is equivalent to a frequentist approach with a flat or uniform prior but the question arises: Do bayesian Credible Intervals share the properties of frequentist Confidence Intervals, given a flat / uniform prior?\nThe question arises from a post and discussion with Geoffrey Johnson, Director at Merck, around January 2025."
  },
  {
    "objectID": "projects/Freq_Bayesian_Binomial/index.html#model-specification-and-interval-calculations",
    "href": "projects/Freq_Bayesian_Binomial/index.html#model-specification-and-interval-calculations",
    "title": "Bayesian vs Frequentist Intervals, Binomial comparison",
    "section": "Model specification and interval calculations",
    "text": "Model specification and interval calculations\nFor the frequentist approach, we will consider an exact binomial test, which is a test of proportions for small samples. If our sample was large enough, we could use Pearson’s \\(\\chi^2\\) test.\nFor the bayesian approach, we will consider 2 alternatives. A Bayesian with \\(\\theta \\sim Beta(1,1)\\) which is flat, and an alternative with \\(\\theta \\sim Beta(0.1,0.1)\\) which is U shaped.\nTo elaborate on the posterior probability, we’re saying:\n\\[\np(\\theta|x) \\propto p(x|\\theta)p(\\theta)\n\\]\nWhich is to say that our joint posterior probability is equal to our likelihood function times our prior for the parameter.\nIn the case of the Beta-Binomial model (Beta prior, Binomial likelihood), the Beta distribution is a conjugate prior for the Binomial, and we get a Beta distribution as a posterior distribution.\nAs a memory refresher, and considering \\(m\\) successes the binomial likelihood looks like \\(\\binom{n}{m}\\theta^m(1 - \\theta)^{n-m}\\), and the beta distribution as our prior would be \\(\\frac{\\theta^{\\alpha-1}(1 - \\theta)^{\\beta-1}}{B(\\alpha, \\beta)}\\), which translates to our beta joint posterior:\n\\[\np(\\theta|x) \\propto \\frac{\\theta^{\\alpha + m -1}(1 - \\theta)^{\\beta + n - m - 1}}{B(\\alpha + m, \\beta + n - m)}\n\\]\nWhere \\(B(\\alpha, \\beta)\\) is the Beta function.\nNotice our hyperparameters \\(\\alpha\\) and \\(\\beta\\), which are set up by the bayesian statistician. In our two scenarios, they will take the values 1 and 0.1 each.\nFor this simulation, we’ll consider a 90% Interval, which means our threshold for a two sided interval is 0.05 (instead of 0.5/2, which would be 95% confidence)\nThis are our 2 prior distributions for our parameter \\(\\theta\\).\n\n\n\n\n\n\n\n\n\nAfter running our 100.000 simulations, we can calculate the average coverage that we’ll get out of our test and realize that for small sample sizes, the frequentist approach has almost 3% extra coverage than we specified, which is expected for small samples.\n\n\n\n\n\nFrequentist Coverage\nFlat Beta Coverage\nU-shaped Beta Coverage\n\n\n\n\n0.92646\n0.85907\n0.92646\n\n\n\n\n\nOn the other hand, the uniform beta prior distribution has less than 90% coverage, due to the small sample size. This is one of the reasons why studying prior distribution behaviour and posterior properties through simulation is useful. If we want to achieve frequentist coverage levels, we would need to adjust our hyperparameters."
  },
  {
    "objectID": "projects/Freq_Bayesian_Binomial/index.html#experiment",
    "href": "projects/Freq_Bayesian_Binomial/index.html#experiment",
    "title": "Bayesian vs Frequentist Intervals, Binomial comparison",
    "section": "Experiment",
    "text": "Experiment\nNow, with all previously said. We can study the behaviour of a single experiment and see how the bayesian posterior probability and the p value behave, when our theta is known, and we have a single experiment at \\(n=10\\) and \\(m=7\\).\nWe can see that with our fixed hyperparameters for the flat uniform beta, or the U-shaped beta the posterior probability changes We can also see that the frequentist interval tends to be wider than any of the two bayesian credible intervals.\n\n\n\n\n\n\n\n\n\nAs a summary, frequentist intervals have the great property given through all the hard work and math behind their creation, where their coverage is often guaranteed when assumptions are met. Bayesian Intervals, do not have that property but certain prior distribution hyperparameters can induce enough uncertainty to match the coverage of a frequentist interval, but with a reduction in uncertainty around the parameter, as well as having the advantage of exact inference, due to the use of probability distributions.\nNotice how the U-shaped beta prior model peaks closer to the true value for \\(\\theta=0.74\\), while maintaining same coverage and inducing less uncertainty.\nThe code for the simulation can be found in my GH Pages Repository."
  },
  {
    "objectID": "projects/P1_Symmetric_Conformal_Prediction/index.html",
    "href": "projects/P1_Symmetric_Conformal_Prediction/index.html",
    "title": "Conformal Prediction Part 1: Symmetric intervals",
    "section": "",
    "text": "This is the first part of a small series to show how to implement Conformal Prediction intervals in R from scratch, without relying in frameworks (such as Tidymodels) or other specific packages. Initially, quick and easy reproducible posts will be done, and later I’ll use some real-world examples.\nThis particular case is based on a regression problem, where we need a symmetric interval with a 95% coverage guarantee around a non-linear estimator of the expected value, using a simple neural network.\nThe focus of this series is not on the complexity of the methods or models, but rather to exemplify how to implement conformal intervals with a specific guarantee.\n\n\nConformal Prediction (CP) is an uncertainty quantification framework or techniques used in Machine Learning problems where classical asymptotic theory does not hold too well due to the type of models being used. The most common use is split conformal prediction, where from our training data \\(D\\) we split it into \\(D_1\\) proper training data and \\(D_2\\) for calibration data.\nThe objective is to create an interval with an \\(\\alpha\\) coverage level guarantee that is distribution-free, using a function \\(\\hat f_n(x)\\), where we’ll predict the value of \\(y\\) at \\(x\\). This could be the conditional expectation of a linear model, generalized linear model, a spline, a random forest or even a bayesian model. It could be any function.\nSome nomenclature to be consistent:\n\nTraining data: \\(D_1\\)\nCalibration data: \\(D_2\\)\nFunction (or adjusted model): \\(\\hat f_{n_1}(x)\\)\nResidual: \\(R_i\\)\n\nSome questions might arise, such as: Why not just use the residuals from the training set to build our intervals? Well, the quick answer to that is that since the residuals used to build the model, they are not independent with respect to our model, so the prediction intervals we build from those residuals might not give us the coverage guarantee we want.\nFor this problem, we can use around 1000 data points from a calibration set (hence, the name split conformal prediction).\nWe’ll use the following libraries for this example:\n\n\nCode\nlibrary(dplyr)\nlibrary(neuralnet)\nlibrary(ggplot2)\n\n\nWe’ll also create a function to simulate data. This function is taken from Tidymodel’s Conformal Prediction post, but we won’t be using any function from the Tidymodels framework.\n\n\nCode\nmake_data &lt;- function(n, std_dev = 1 / 5) {\n  tibble::tibble(x = runif(n, min = -1))  |&gt; \n    dplyr::mutate(\n      y = (x^3) + 2 * exp(-6 * (x - 0.3)^2),\n      y = y + rnorm(n, sd = std_dev)\n    )\n}\n\n\nI will pre-create our different sets: training, calibration, and test.\n\n\nCode\nset.seed(8383)\n\ntrain_data &lt;- make_data(10000)\ncal_data &lt;- make_data(1000)\ntest_data &lt;- make_data(10000)\n\n\nAnd, our training data looks like this! As you can see, a simple linear regression will probably not work. We could do something using smooth splines, or maybe a tree-based method but I’ll be using a neural network to use a commonly used ML model.\n\n\n\n\n\n\n\n\n\nOur function \\(\\hat f_{n_1}(x_i), \\ i \\in D_1\\), trained predicts the overall mean well enough.\n\n\nCode\nm_nnet &lt;- neuralnet(y ~ x , train_data, hidden = 4)\n\n\n\n\n\n\n\n\n\n\n\nTo create our prediction intervals, we need to define a ‘distance’ or ‘score’ function for our calibration set residuals \\(R_i\\), \\(i \\in D_2\\). We can use any negatively oriented function (where lower values are better!).\nWe’ll be using the absolute difference: \\(R_i = |Y_i - \\hat f_{n_1}(X_i)|,\\ i \\in D_2\\) for this example since it’s easy to understand, and it maintains the scale of the original problem.\nOur \\(\\alpha\\) level for our intervals will be 0.05, and our conformal quantile is calculated \\(\\hat q_{n_2} = \\lceil(1 - \\alpha)(n_2 + 1)\\rceil\\).\nSo, since calibration set has 1.000 data points, the rank we’re looking for would be: 951, since this is rank, we’ll need to find the residual that matches this particular rank.\nWe could alternatively divide this index by the number of rows (\\(n_2\\)) of our calibration set, and look for the quantile which you’ll see gives a very similar result.\n\n\nCode\n# Fit model to calibration set\ncal_fit &lt;- predict(m_nnet, cal_data[,1]) \n\n# Calculate score\nres &lt;- abs(cal_fit - cal_data[,\"y\"])\n# Find the R_i\nR_i &lt;- ceiling(0.95 * (nrow(cal_data)+1))\n\n# Rank method\nrank_value &lt;- res$y[rank(res) == R_i]\n# Quantile method\nquantile_value &lt;- quantile(res$y, R_i/nrow(res))\n\n\n\n\n\n\n\nMethod\nValue\n\n\n\n\nRank method\n0.4127800\n\n\nQuantile method\n0.4129021\n\n\n\n\n\nNow, the beauty of this is that the conformal quantile/rank we just calculated, will give us a guaranteed \\(\\alpha\\) level prediction interval as \\(n \\rightarrow \\infty\\) in our calibration set, so the bigger our calibration set the closer we’ll get to that coverage. Thus, it’s often common to split around 1000 observations or more, which means this method is better suited for big data than small sample sizes.\n\n\nCode\n# Fit to test data\ntest_fit &lt;- predict(m_nnet, test_data[,1])\n\n\n\n\n\n\n\n\n\n\n\nSo, using our test set of 10.000 observations, we can see that the coverage is actually at least 95 %.\nThis is extremely relevant, since previously most Machine Learning methods lacked a reliable way to measure the uncertainty of their methods, and relying on asymptotic normality does not always make sense for a problem.\n\n\nCode\n# Create dataframe with relevant data\nintervals &lt;- data.frame(lower = test_fit - rank_value,\n                        upper = test_fit + rank_value,\n                        pred = test_fit,\n                        y = test_data$y)\n\n# Calculate coverage\nintervals |&gt; \n  dplyr::mutate(coverage = ifelse(y &lt;= upper & y &gt;= lower, 1 , 0)) |&gt; \n  summarise(coverage = round(sum(coverage)/ length(coverage), 2)) |&gt; \n  knitr::kable(col.names = c(\"Coverage\"))\n\n\n\n\n\nCoverage\n\n\n\n\n0.96"
  },
  {
    "objectID": "projects/P1_Symmetric_Conformal_Prediction/index.html#about-conformal-prediction",
    "href": "projects/P1_Symmetric_Conformal_Prediction/index.html#about-conformal-prediction",
    "title": "Conformal Prediction Part 1: Symmetric intervals",
    "section": "",
    "text": "Conformal Prediction (CP) is an uncertainty quantification framework or techniques used in Machine Learning problems where classical asymptotic theory does not hold too well due to the type of models being used. The most common use is split conformal prediction, where from our training data \\(D\\) we split it into \\(D_1\\) proper training data and \\(D_2\\) for calibration data.\nThe objective is to create an interval with an \\(\\alpha\\) coverage level guarantee that is distribution-free, using a function \\(\\hat f_n(x)\\), where we’ll predict the value of \\(y\\) at \\(x\\). This could be the conditional expectation of a linear model, generalized linear model, a spline, a random forest or even a bayesian model. It could be any function.\nSome nomenclature to be consistent:\n\nTraining data: \\(D_1\\)\nCalibration data: \\(D_2\\)\nFunction (or adjusted model): \\(\\hat f_{n_1}(x)\\)\nResidual: \\(R_i\\)\n\nSome questions might arise, such as: Why not just use the residuals from the training set to build our intervals? Well, the quick answer to that is that since the residuals used to build the model, they are not independent with respect to our model, so the prediction intervals we build from those residuals might not give us the coverage guarantee we want.\nFor this problem, we can use around 1000 data points from a calibration set (hence, the name split conformal prediction).\nWe’ll use the following libraries for this example:\n\n\nCode\nlibrary(dplyr)\nlibrary(neuralnet)\nlibrary(ggplot2)\n\n\nWe’ll also create a function to simulate data. This function is taken from Tidymodel’s Conformal Prediction post, but we won’t be using any function from the Tidymodels framework.\n\n\nCode\nmake_data &lt;- function(n, std_dev = 1 / 5) {\n  tibble::tibble(x = runif(n, min = -1))  |&gt; \n    dplyr::mutate(\n      y = (x^3) + 2 * exp(-6 * (x - 0.3)^2),\n      y = y + rnorm(n, sd = std_dev)\n    )\n}\n\n\nI will pre-create our different sets: training, calibration, and test.\n\n\nCode\nset.seed(8383)\n\ntrain_data &lt;- make_data(10000)\ncal_data &lt;- make_data(1000)\ntest_data &lt;- make_data(10000)\n\n\nAnd, our training data looks like this! As you can see, a simple linear regression will probably not work. We could do something using smooth splines, or maybe a tree-based method but I’ll be using a neural network to use a commonly used ML model.\n\n\n\n\n\n\n\n\n\nOur function \\(\\hat f_{n_1}(x_i), \\ i \\in D_1\\), trained predicts the overall mean well enough.\n\n\nCode\nm_nnet &lt;- neuralnet(y ~ x , train_data, hidden = 4)\n\n\n\n\n\n\n\n\n\n\n\nTo create our prediction intervals, we need to define a ‘distance’ or ‘score’ function for our calibration set residuals \\(R_i\\), \\(i \\in D_2\\). We can use any negatively oriented function (where lower values are better!).\nWe’ll be using the absolute difference: \\(R_i = |Y_i - \\hat f_{n_1}(X_i)|,\\ i \\in D_2\\) for this example since it’s easy to understand, and it maintains the scale of the original problem.\nOur \\(\\alpha\\) level for our intervals will be 0.05, and our conformal quantile is calculated \\(\\hat q_{n_2} = \\lceil(1 - \\alpha)(n_2 + 1)\\rceil\\).\nSo, since calibration set has 1.000 data points, the rank we’re looking for would be: 951, since this is rank, we’ll need to find the residual that matches this particular rank.\nWe could alternatively divide this index by the number of rows (\\(n_2\\)) of our calibration set, and look for the quantile which you’ll see gives a very similar result.\n\n\nCode\n# Fit model to calibration set\ncal_fit &lt;- predict(m_nnet, cal_data[,1]) \n\n# Calculate score\nres &lt;- abs(cal_fit - cal_data[,\"y\"])\n# Find the R_i\nR_i &lt;- ceiling(0.95 * (nrow(cal_data)+1))\n\n# Rank method\nrank_value &lt;- res$y[rank(res) == R_i]\n# Quantile method\nquantile_value &lt;- quantile(res$y, R_i/nrow(res))\n\n\n\n\n\n\n\nMethod\nValue\n\n\n\n\nRank method\n0.4127800\n\n\nQuantile method\n0.4129021\n\n\n\n\n\nNow, the beauty of this is that the conformal quantile/rank we just calculated, will give us a guaranteed \\(\\alpha\\) level prediction interval as \\(n \\rightarrow \\infty\\) in our calibration set, so the bigger our calibration set the closer we’ll get to that coverage. Thus, it’s often common to split around 1000 observations or more, which means this method is better suited for big data than small sample sizes.\n\n\nCode\n# Fit to test data\ntest_fit &lt;- predict(m_nnet, test_data[,1])\n\n\n\n\n\n\n\n\n\n\n\nSo, using our test set of 10.000 observations, we can see that the coverage is actually at least 95 %.\nThis is extremely relevant, since previously most Machine Learning methods lacked a reliable way to measure the uncertainty of their methods, and relying on asymptotic normality does not always make sense for a problem.\n\n\nCode\n# Create dataframe with relevant data\nintervals &lt;- data.frame(lower = test_fit - rank_value,\n                        upper = test_fit + rank_value,\n                        pred = test_fit,\n                        y = test_data$y)\n\n# Calculate coverage\nintervals |&gt; \n  dplyr::mutate(coverage = ifelse(y &lt;= upper & y &gt;= lower, 1 , 0)) |&gt; \n  summarise(coverage = round(sum(coverage)/ length(coverage), 2)) |&gt; \n  knitr::kable(col.names = c(\"Coverage\"))\n\n\n\n\n\nCoverage\n\n\n\n\n0.96"
  },
  {
    "objectID": "projects/P2_Asymmetric_Conformal_Prediction/index.html",
    "href": "projects/P2_Asymmetric_Conformal_Prediction/index.html",
    "title": "Conformal Prediction Part 2: Asymmetric intervals",
    "section": "",
    "text": "This is the second part of a small series to show how to implement Conformal Prediction intervals in R from scratch, without relying in frameworks (such as Tidymodels) or other specific packages. Initially, quick and easy reproducible posts will be done, and later I’ll use some real-world examples.\nThis particular case is based on a heteroscedastic regression problem, where we need an asymmetric interval with a 95% coverage guarantee around a non-linear estimator of the expected value, using a simple neural network.\nThe focus of this series is not on the complexity of the methods or models, but rather to exemplify how to implement conformal intervals with a specific guarantee.\n\n\nConformal Prediction (CP) is an uncertainty quantification framework or techniques used in Machine Learning problems where classical asymptotic theory does not hold too well due to the type of models being used. The most common use is split conformal prediction, where from our training data \\(D\\) we split it into \\(D_1\\) proper training data and \\(D_2\\) for calibration data.\nThe objective is to create an interval with an \\(\\alpha\\) coverage level guarantee that is distribution-free, using a function \\(\\hat f_n(x)\\), where using the train data, we’ll train a neural network to predict the mean, or point predictor of the regression at \\(x\\), and a second model will be fit on the ‘spread’ of the training residuals, and used to predict the dispersion on the calibration data. The model used to predict the conditional expectation or conditional mean could be a linear model, generalized linear model, a spline, a random forest or even a bayesian model. It could be any function.\nSome nomenclature to be consistent:\n\nTraining data: \\(D_1\\)\nCalibration data: \\(D_2\\)\nPoint predictor model: \\(\\hat f_{n_1}(X_i)\\)\nSpread predictor model: \\(\\hat \\sigma_{n_1}(X_i)\\)\nResidual: \\(R_i\\)\n\nFor this problem, we can use around 1000 data points from a calibration set (hence, the name split conformal prediction).\nWe’ll use the following libraries for this example:\n\n\nCode\nlibrary(dplyr)\nlibrary(neuralnet)\nlibrary(ggplot2)\n\n\nWe’ll also create a function to simulate data. This function is taken from Tidymodel’s Conformal Prediction post, but we won’t be using any function from the Tidymodels framework.\n\n\nCode\nmake_variable_data &lt;- function(n, std_dev = 1 / 5) {\n  tibble(x = runif(n, min = -1))  |&gt; \n    mutate(\n      y = (x^3) + 2 * exp(-6 * (x - 0.3)^2),\n      y = y + rnorm(n, sd = std_dev * abs(x))\n    )\n}\n\ncoverage &lt;- function(y, lower, upper){\n  aux &lt;- ifelse(y &gt;= lower & y &lt;= upper, 1 , 0)\n  sum(aux)/length(aux)\n  \n}\n\n\nI will pre-create our different sets: training, calibration, and test.\n\n\nCode\nset.seed(7292)\ntrain_variable_data &lt;- make_variable_data(10000)\ncal_variable_data &lt;- make_variable_data(1000)\ntest_variable_data &lt;- make_variable_data(10000)\n\n\nAnd, our training data looks like this! As you can see, a simple linear regression will probably not work too well. We could do something using smooth splines, or maybe a tree-based method but I’ll be using a neural network to use a commonly used ML model.\n\n\n\n\n\n\n\n\n\nOur function \\(\\hat f_{n_1}(X_i), \\ i \\in D_1\\), trained predicts the overall mean well enough. The best fit line looks good, despite having regions with higher and lower heteroskedasticity.\n\n\nCode\nm_nnet &lt;- \n  neuralnet(y ~ x,\n            train_variable_data,\n            hidden = c(4,5),\n            threshold=0.04, \n            act.fct=\"tanh\", \n            linear.output=TRUE, \n            stepmax=1e7)\n\n\n\n\n\n\n\n\n\n\n\nTo create our prediction intervals, we need to define a ‘distance’ or ‘score’ function and a spread predictor. We’ll be using the absolute difference for the score function, and the function for the spread will be based on this score.\nThe spread predictor should be estimated using the same variables as the neural network, not necessarily using the same type of model. In this particular case we only have 1 covariate. To show that you can use something other than a neural network, my spread predictor will be a smoothing spline. The relationship between our predictor and the score function looks like this, where the red line is \\(\\hat \\sigma_{n_1}\\).\n\n\nCode\n# Fit model to calibration set\nspread &lt;- abs(train_variable_data$y - m_nnet$net.result[[1]])\nspread_predictor &lt;- smooth.spline(train_variable_data$x, spread)\n\n\n\n\n\n\n\n\n\n\n\nNow, we need calculate our calibration studentized residuals \\(R_i = \\frac{|Y_i - \\hat f_{n_1}(X_i)|}{\\hat\\sigma_{n_1}(X_i)}, \\ i \\in D_2\\), using our calibration data.\n\n\nCode\ncal_spread &lt;- predict(spread_predictor, cal_variable_data[,1])\ncal_fit &lt;- predict(m_nnet, cal_variable_data[,1]) \n\n\nquantile_searched &lt;- ceiling(0.95 * (nrow(cal_variable_data)+1))/nrow(cal_variable_data)\n\n# Find the R_i\nr_i &lt;- abs(cal_variable_data$y - cal_fit[,1])\nr_i &lt;- r_i/cal_spread$y$x \n\nq_var &lt;- quantile(r_i, quantile_searched)\n\n\nIn order to get our prediction intervals, we just need to multiply our spread predictor by our estimated quantile value, and create an lower and upper bound around the point predictor: \\(\\hat C_n(x) = \\left[\\hat f_{n_1}(x) - \\hat \\sigma_{n_1}(x)\\hat q_{n_2}, \\ \\hat f_{n_1}(x) + \\hat \\sigma_{n_1}(x)\\hat q_{n_2} \\right]\\).\nThis, in our calibration set looks like this:\n\n\n\n\n\n\n\n\n\nNow we can calculate our prediction intervals in our test set, and check the coverage of our method.\n\n\nCode\n# Fit to test data\ntest_fit &lt;- predict(m_nnet, test_variable_data[,1])\ntest_spread &lt;- predict(spread_predictor, test_variable_data[,1])\nlower_bounds &lt;- test_fit - test_spread$y$x*q_var\nupper_bounds &lt;- test_fit + test_spread$y$x*q_var\ntest_coverage &lt;- coverage(test_variable_data[,\"y\"],\n                          lower_bounds,\n                          upper_bounds)\n\n\n\n\nCode\n# Plot the symmetric intervals\nggplot2::ggplot(aes(x, y), data = test_variable_data) + \n  ggplot2::geom_point(alpha = 1 / 15)  +\n  ggplot2::geom_line(aes(x, test_fit), col = \"darkred\",\n                     linewidth = 1) + \n  geom_ribbon(aes(x = x, ymin = lower_bounds,\n                  ymax = upper_bounds), alpha = 1/4,\n              fill = \"pink\") + \n  annotate(\"text\",\n           x = 0.9, y = -1.5, \n           label = paste(\"Coverage of:\", \n                         scales::percent(test_coverage) ))\n\n\n\n\n\n\n\n\n\nSo, using our test set of 10.000 observations, we can see that the coverage is actually at least 95 % using studentized residuals even on the case of heteroscedastic data. This is fairly useful, especially when you see a pattern that can be modeled by a function within the training residuals.\nStudentized residuals might fall short when \\(\\hat f_{n_1}\\) is complex and leaves little to no information on the residuals to use this method. An alternative to this is a conformalized quantile regression, a method that focuses directly on the prediction bands instead of using residuals. This I’ll cover on part 3.\nThe idea of most conformal prediction methods is similar: to make post-hoc adjustments to our uncertainty in order to achieve a specific long-run coverage, which is firmly rooted in a frequentist interpretation of statistics."
  },
  {
    "objectID": "projects/P2_Asymmetric_Conformal_Prediction/index.html#about-conformal-prediction",
    "href": "projects/P2_Asymmetric_Conformal_Prediction/index.html#about-conformal-prediction",
    "title": "Conformal Prediction Part 2: Asymmetric intervals",
    "section": "",
    "text": "Conformal Prediction (CP) is an uncertainty quantification framework or techniques used in Machine Learning problems where classical asymptotic theory does not hold too well due to the type of models being used. The most common use is split conformal prediction, where from our training data \\(D\\) we split it into \\(D_1\\) proper training data and \\(D_2\\) for calibration data.\nThe objective is to create an interval with an \\(\\alpha\\) coverage level guarantee that is distribution-free, using a function \\(\\hat f_n(x)\\), where using the train data, we’ll train a neural network to predict the mean, or point predictor of the regression at \\(x\\), and a second model will be fit on the ‘spread’ of the training residuals, and used to predict the dispersion on the calibration data. The model used to predict the conditional expectation or conditional mean could be a linear model, generalized linear model, a spline, a random forest or even a bayesian model. It could be any function.\nSome nomenclature to be consistent:\n\nTraining data: \\(D_1\\)\nCalibration data: \\(D_2\\)\nPoint predictor model: \\(\\hat f_{n_1}(X_i)\\)\nSpread predictor model: \\(\\hat \\sigma_{n_1}(X_i)\\)\nResidual: \\(R_i\\)\n\nFor this problem, we can use around 1000 data points from a calibration set (hence, the name split conformal prediction).\nWe’ll use the following libraries for this example:\n\n\nCode\nlibrary(dplyr)\nlibrary(neuralnet)\nlibrary(ggplot2)\n\n\nWe’ll also create a function to simulate data. This function is taken from Tidymodel’s Conformal Prediction post, but we won’t be using any function from the Tidymodels framework.\n\n\nCode\nmake_variable_data &lt;- function(n, std_dev = 1 / 5) {\n  tibble(x = runif(n, min = -1))  |&gt; \n    mutate(\n      y = (x^3) + 2 * exp(-6 * (x - 0.3)^2),\n      y = y + rnorm(n, sd = std_dev * abs(x))\n    )\n}\n\ncoverage &lt;- function(y, lower, upper){\n  aux &lt;- ifelse(y &gt;= lower & y &lt;= upper, 1 , 0)\n  sum(aux)/length(aux)\n  \n}\n\n\nI will pre-create our different sets: training, calibration, and test.\n\n\nCode\nset.seed(7292)\ntrain_variable_data &lt;- make_variable_data(10000)\ncal_variable_data &lt;- make_variable_data(1000)\ntest_variable_data &lt;- make_variable_data(10000)\n\n\nAnd, our training data looks like this! As you can see, a simple linear regression will probably not work too well. We could do something using smooth splines, or maybe a tree-based method but I’ll be using a neural network to use a commonly used ML model.\n\n\n\n\n\n\n\n\n\nOur function \\(\\hat f_{n_1}(X_i), \\ i \\in D_1\\), trained predicts the overall mean well enough. The best fit line looks good, despite having regions with higher and lower heteroskedasticity.\n\n\nCode\nm_nnet &lt;- \n  neuralnet(y ~ x,\n            train_variable_data,\n            hidden = c(4,5),\n            threshold=0.04, \n            act.fct=\"tanh\", \n            linear.output=TRUE, \n            stepmax=1e7)\n\n\n\n\n\n\n\n\n\n\n\nTo create our prediction intervals, we need to define a ‘distance’ or ‘score’ function and a spread predictor. We’ll be using the absolute difference for the score function, and the function for the spread will be based on this score.\nThe spread predictor should be estimated using the same variables as the neural network, not necessarily using the same type of model. In this particular case we only have 1 covariate. To show that you can use something other than a neural network, my spread predictor will be a smoothing spline. The relationship between our predictor and the score function looks like this, where the red line is \\(\\hat \\sigma_{n_1}\\).\n\n\nCode\n# Fit model to calibration set\nspread &lt;- abs(train_variable_data$y - m_nnet$net.result[[1]])\nspread_predictor &lt;- smooth.spline(train_variable_data$x, spread)\n\n\n\n\n\n\n\n\n\n\n\nNow, we need calculate our calibration studentized residuals \\(R_i = \\frac{|Y_i - \\hat f_{n_1}(X_i)|}{\\hat\\sigma_{n_1}(X_i)}, \\ i \\in D_2\\), using our calibration data.\n\n\nCode\ncal_spread &lt;- predict(spread_predictor, cal_variable_data[,1])\ncal_fit &lt;- predict(m_nnet, cal_variable_data[,1]) \n\n\nquantile_searched &lt;- ceiling(0.95 * (nrow(cal_variable_data)+1))/nrow(cal_variable_data)\n\n# Find the R_i\nr_i &lt;- abs(cal_variable_data$y - cal_fit[,1])\nr_i &lt;- r_i/cal_spread$y$x \n\nq_var &lt;- quantile(r_i, quantile_searched)\n\n\nIn order to get our prediction intervals, we just need to multiply our spread predictor by our estimated quantile value, and create an lower and upper bound around the point predictor: \\(\\hat C_n(x) = \\left[\\hat f_{n_1}(x) - \\hat \\sigma_{n_1}(x)\\hat q_{n_2}, \\ \\hat f_{n_1}(x) + \\hat \\sigma_{n_1}(x)\\hat q_{n_2} \\right]\\).\nThis, in our calibration set looks like this:\n\n\n\n\n\n\n\n\n\nNow we can calculate our prediction intervals in our test set, and check the coverage of our method.\n\n\nCode\n# Fit to test data\ntest_fit &lt;- predict(m_nnet, test_variable_data[,1])\ntest_spread &lt;- predict(spread_predictor, test_variable_data[,1])\nlower_bounds &lt;- test_fit - test_spread$y$x*q_var\nupper_bounds &lt;- test_fit + test_spread$y$x*q_var\ntest_coverage &lt;- coverage(test_variable_data[,\"y\"],\n                          lower_bounds,\n                          upper_bounds)\n\n\n\n\nCode\n# Plot the symmetric intervals\nggplot2::ggplot(aes(x, y), data = test_variable_data) + \n  ggplot2::geom_point(alpha = 1 / 15)  +\n  ggplot2::geom_line(aes(x, test_fit), col = \"darkred\",\n                     linewidth = 1) + \n  geom_ribbon(aes(x = x, ymin = lower_bounds,\n                  ymax = upper_bounds), alpha = 1/4,\n              fill = \"pink\") + \n  annotate(\"text\",\n           x = 0.9, y = -1.5, \n           label = paste(\"Coverage of:\", \n                         scales::percent(test_coverage) ))\n\n\n\n\n\n\n\n\n\nSo, using our test set of 10.000 observations, we can see that the coverage is actually at least 95 % using studentized residuals even on the case of heteroscedastic data. This is fairly useful, especially when you see a pattern that can be modeled by a function within the training residuals.\nStudentized residuals might fall short when \\(\\hat f_{n_1}\\) is complex and leaves little to no information on the residuals to use this method. An alternative to this is a conformalized quantile regression, a method that focuses directly on the prediction bands instead of using residuals. This I’ll cover on part 3.\nThe idea of most conformal prediction methods is similar: to make post-hoc adjustments to our uncertainty in order to achieve a specific long-run coverage, which is firmly rooted in a frequentist interpretation of statistics."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Projects\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nProjects\n\n\n\nDate\n\n\n\nDescription\n\n\n\nCategories\n\n\n\n\n\n\n\n\nBayesian vs Frequentist Intervals, Binomial comparison\n\n\nMar 19, 2025\n\n\nEvaluation of long-run properties of a Binomial test, from a Bayesian and Frequentist perspective\n\n\nR, Statistics, Bayesian\n\n\n\n\n\n\nConformal Prediction Part 1: Symmetric intervals\n\n\nApr 7, 2025\n\n\nCreation of Conformal intervals for uncertainty quantifiction in Machine Learning, example using simulated data\n\n\nR, Statistics, Machine Learning, Conformal Prediction, Uncertainty Quantification\n\n\n\n\n\n\nConformal Prediction Part 2: Asymmetric intervals\n\n\nApr 29, 2025\n\n\nCreation of Conformal intervals for uncertainty quantifiction in Machine Learning under heteroscedasticity, example using simulated data\n\n\nR, Statistics, Machine Learning, Conformal Prediction, Uncertainty Quantification\n\n\n\n\n\n\nForecasting: Banco de Chile’s Consumer Stock\n\n\nMar 19, 2025\n\n\nForecasting through classic SARIMA and SARIMAX models\n\n\nR, Statistics, Time Series, Forecasting\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]