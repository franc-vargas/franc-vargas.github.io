[
  {
    "objectID": "projects/Banco de Chile Consumer Stock/index.html",
    "href": "projects/Banco de Chile Consumer Stock/index.html",
    "title": "Forecasting: Banco de Chile’s Consumer Stock",
    "section": "",
    "text": "The normal consumer stock (NCS) of a bank represents the amount of money that regular consumers invest in consumer credit. In this particular case, it is measured in Chilean Pesos (CLP). In this particular case, monthly NCS values were downloaded from xxxxx time series data bank. We also scraped the Monthy Index of Economic Activity (IMACEC), and the Interest Rate of the Central Bank of Chile.\n\n\nWe can talk about stationarity in time series in two ways, stationarity of the mean and stationarity of the variance. Stationarity of the mean means that the values are around a mean \\(\\mu\\), or a fixed value, while variance means that the spread around that mean is even and there’s no periods where it increases or decreases. Many times this assumptions are not met, which means that we need to evaluate our time series a bit differently. We can assess the stationarity of the mean and variance of the first difference, and see if the mean stabilizes and then if the variance continues to be heteroskedastic, we can apply a transformation (sqrt, log, etc) to the original series and then evaluate the first difference.\nTo assess this, the quickest and simplest way to do is to look at a graph of the series and, as we can see here, this is a non-stationary time-series, both in variance and mean.\n\n\n\n\n\n\n\n\n\nIn this particular case, variance was stabilized via log-transformation, and the first difference was stationary in mean:\n\n\n\n\n\n\n\n\n\nWe can see that despite transformations and first-difference there are some breaks on our time series, which will influence the outcome of the classical SARIMA model.\n\n\n\nACF and PACF plots helps us decide the initial values for the SARIMA part of the model, while CCF helps us check the extent of the correlation between our covariates that we’ll use for forecasting.\n\n\n\n\n\n\n\n\n\nOur PACF and ACF plots suggest an SARIMA structure, we’ll consider an ARIMA (3,1,1) and a SARMA (3,0,1) and since this is monthly data we’ll consider quarterly seasonality.\nFor the SARIMAX model, same ARIMA and SARMA parts were considered, but our covariates included the ln of IMACEC and ln of the Interest Rate for Consumer credits, both lagged by 12 months. This decision came from noticing the CCF plot which showed relevant correlations 12 months in the past, as well as noticing that only the marginal effects (or interaction) between the two covariates was the most relevant part.\nSince the lagged product was the most relevant part, we modeled the product as the sum of logarithms.\n\n\n\nWe can see that while the SARIMA model maintains the proper shape, but the trend is off. In the case of the SARIMAX model, our covariates fix that trend through a linear regression.\n\n\n\n\n\n\nAnd our model evaluation, which considering we’re only using two covariates + time, is relatively good.\n\n\n\n\n\nModel\nRMSE\nMAE\nMPE\nMAPE\nMASE\nME\n\n\n\n\nSARIMA\n125657.04\n117211.93\n-2.4%\n2.4%\n1.98236\n-117211.93\n\n\nSARIMAX\n41968.47\n34211.05\n-0.5%\n0.7%\n0.57860\n-24576.02"
  },
  {
    "objectID": "projects/Banco de Chile Consumer Stock/index.html#stationarity",
    "href": "projects/Banco de Chile Consumer Stock/index.html#stationarity",
    "title": "Forecasting: Banco de Chile’s Consumer Stock",
    "section": "",
    "text": "We can talk about stationarity in time series in two ways, stationarity of the mean and stationarity of the variance. Stationarity of the mean means that the values are around a mean \\(\\mu\\), or a fixed value, while variance means that the spread around that mean is even and there’s no periods where it increases or decreases. Many times this assumptions are not met, which means that we need to evaluate our time series a bit differently. We can assess the stationarity of the mean and variance of the first difference, and see if the mean stabilizes and then if the variance continues to be heteroskedastic, we can apply a transformation (sqrt, log, etc) to the original series and then evaluate the first difference.\nTo assess this, the quickest and simplest way to do is to look at a graph of the series and, as we can see here, this is a non-stationary time-series, both in variance and mean.\n\n\n\n\n\n\n\n\n\nIn this particular case, variance was stabilized via log-transformation, and the first difference was stationary in mean:\n\n\n\n\n\n\n\n\n\nWe can see that despite transformations and first-difference there are some breaks on our time series, which will influence the outcome of the classical SARIMA model."
  },
  {
    "objectID": "projects/Banco de Chile Consumer Stock/index.html#auto-correlations-partial-auto-correlations-cross-correlations",
    "href": "projects/Banco de Chile Consumer Stock/index.html#auto-correlations-partial-auto-correlations-cross-correlations",
    "title": "Forecasting: Banco de Chile’s Consumer Stock",
    "section": "",
    "text": "ACF and PACF plots helps us decide the initial values for the SARIMA part of the model, while CCF helps us check the extent of the correlation between our covariates that we’ll use for forecasting.\n\n\n\n\n\n\n\n\n\nOur PACF and ACF plots suggest an SARIMA structure, we’ll consider an ARIMA (3,1,1) and a SARMA (3,0,1) and since this is monthly data we’ll consider quarterly seasonality.\nFor the SARIMAX model, same ARIMA and SARMA parts were considered, but our covariates included the ln of IMACEC and ln of the Interest Rate for Consumer credits, both lagged by 12 months. This decision came from noticing the CCF plot which showed relevant correlations 12 months in the past, as well as noticing that only the marginal effects (or interaction) between the two covariates was the most relevant part.\nSince the lagged product was the most relevant part, we modeled the product as the sum of logarithms."
  },
  {
    "objectID": "projects/Banco de Chile Consumer Stock/index.html#evaluation",
    "href": "projects/Banco de Chile Consumer Stock/index.html#evaluation",
    "title": "Forecasting: Banco de Chile’s Consumer Stock",
    "section": "",
    "text": "We can see that while the SARIMA model maintains the proper shape, but the trend is off. In the case of the SARIMAX model, our covariates fix that trend through a linear regression.\n\n\n\n\n\n\nAnd our model evaluation, which considering we’re only using two covariates + time, is relatively good.\n\n\n\n\n\nModel\nRMSE\nMAE\nMPE\nMAPE\nMASE\nME\n\n\n\n\nSARIMA\n125657.04\n117211.93\n-2.4%\n2.4%\n1.98236\n-117211.93\n\n\nSARIMAX\n41968.47\n34211.05\n-0.5%\n0.7%\n0.57860\n-24576.02"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Projects\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nProjects\n\n\nDescription\n\n\nCategories\n\n\n\n\n\n\nBayesian vs Frequentist Intervals, Binomial comparison\n\n\nEvaluation of long-run properties of a Binomial test, from a Bayesian and Frequentist perspective\n\n\nR, Statistics, Time Series, Forecasting\n\n\n\n\nForecasting: Banco de Chile’s Consumer Stock\n\n\nForecasting through classic SARIMA and SARIMAX models\n\n\nR, Statistics, Time Series, Forecasting\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Francisco Vargas",
    "section": "",
    "text": "Pontificia Universidad Católica de Chile | Masters in Statistics | March 2023 - December 2024\nPontificia Universidad Católica de Chile | Diploma in Biostatistics | May 2022 - December 2022\nUniversidad Mayor | Veterinarian | March 2011 - December 2016"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Francisco Vargas",
    "section": "",
    "text": "Pontificia Universidad Católica de Chile | Masters in Statistics | March 2023 - December 2024\nPontificia Universidad Católica de Chile | Diploma in Biostatistics | May 2022 - December 2022\nUniversidad Mayor | Veterinarian | March 2011 - December 2016"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Francisco Vargas",
    "section": "Experience",
    "text": "Experience\nIQVIA | Data Management Analyst | June 2022 - Present\nVirbac Chile | Clinical Trials Assistant | May 2021 - May 2022\nMowi Chile S.A. | Saltwater Site Assistant | September 2019 - January 2021\nMulti X S.A. | Saltwater Site Assistant | June 2018 - September 2019\nAquaChile S.A. | Saltwater Site Assistant | March 2018 - June 2018"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "projects/Freq_Bayesian_Binomial/index.html",
    "href": "projects/Freq_Bayesian_Binomial/index.html",
    "title": "Bayesian vs Frequentist Intervals, Binomial comparison",
    "section": "",
    "text": "Classic frequentist statistics is the main type of statistics that we’re taught at school and on most undergraduate programs, and for good reason. Frequentist statistics assume the parameter of interest is fixed and base their results on long-run events or samples. When creating a confidence interval at 95%, we’re saying that 95% of the intervals produced contain the true mean.\nBayesian interpretation of probability on the other hand, assumes that the parameter itself is a random variable that comes from a probability distribution. The Bayesian statistician will have to assume a prior probability for the parameter, a likelihood function and that will give a posterior probability where we can calculate a posteriori Credible Interval for our parameter of interest.\nIt is often said that the bayesian approach to statistics is equivalent to a frequentist approach with a flat or uniform prior but the question arises: Do bayesian Credible Intervals share the properties of frequentist Confidence Intervals, given a flat / uniform prior?\nThe question arises from a post from Geoffrey Johnson, Director at Merck around January 2025."
  },
  {
    "objectID": "projects/Freq_Bayesian_Binomial/index.html#model-specification-and-interval-calculations",
    "href": "projects/Freq_Bayesian_Binomial/index.html#model-specification-and-interval-calculations",
    "title": "Bayesian vs Frequentist Intervals, Binomial comparison",
    "section": "Model specification and interval calculations",
    "text": "Model specification and interval calculations\nFor the frequentist approach, we will consider an exact binomial test, which is a test of proportions for small samples. If our sample was large enough, we could use Pearson’s \\(\\chi^2\\) test.\nFor the bayesian approach, we will consider 2 alternatives. A Bayesian with \\(\\theta \\sim Beta(1,1)\\) which is flat, and an alternative with \\(\\theta \\sim Beta(0.1,0.1)\\) which is U shaped.\nTo elaborate on the posterior probability, we’re saying:\n\\[\np(\\theta|x) \\propto p(x|\\theta)p(\\theta)\n\\]\nWhich is to say that our joint posterior probability is equal to our likelihood function times our prior for the parameter.\nIn the case of the Beta-Binomial model (Beta prior, Binomial likelihood), the Beta distribution is a conjugate prior for the Binomial, and we get a Beta distribution as a posterior distribution.\nAs a memory refresher, and considering \\(m\\) successes the binomial likelihood looks like \\(\\binom{n}{m}\\theta^m(1 - \\theta)^{n-m}\\), and the beta distribution as our prior would be \\(\\frac{\\theta^{\\alpha-1}(1 - \\theta)^{\\beta-1}}{B(\\alpha, \\beta)}\\), which translates to our beta joint posterior:\n\\[\np(\\theta|x) \\propto \\frac{\\theta^{\\alpha + m -1}(1 - \\theta)^{\\beta + n - m - 1}}{B(\\alpha + m, \\beta + n - m)}\n\\]\nWhere \\(B(\\alpha, \\beta)\\) is the Beta function.\nNotice our hyperparameters \\(\\alpha\\) and \\(\\beta\\), which are set up by the bayesian statistician. In our two scenarios, they will take the values 1 and 0.1 each.\nFor this simulation, we’ll consider a 90% Interval, which means our threshold for a two sided interval is 0.05 (instead of 0.5/2, which would be 95% confidence)\nThis are our 2 prior distributions for our parameter \\(\\theta\\).\n\n\n\n\n\n\n\n\n\nAfter running our 100.000 simulations, we can calculate the average coverage that we’ll get out of our test and realize that for small sample sizes, the frequentist approach has almost 3% extra coverage than we specified, which is expected for small samples.\nOn the other hand, the uniform beta prior distribution has less than 90% coverage, due to the small sample size. This is one of the reasons why studying prior distribution behaviour and posterior properties through simulation is useful. If we want to achieve frequentist coverage levels, we would need to adjust our hyperparameters.\n\n\n\n\n\nFrequentist Coverage\nFlat Beta Coverage\nU-shaped Beta Coverage\n\n\n\n\n0.92646\n0.85907\n0.92646"
  },
  {
    "objectID": "projects/Freq_Bayesian_Binomial/index.html#experiment",
    "href": "projects/Freq_Bayesian_Binomial/index.html#experiment",
    "title": "Bayesian vs Frequentist Intervals, Binomial comparison",
    "section": "Experiment",
    "text": "Experiment\nNow, with all previously said. We can study the behaviour of a single experiment and see how the bayesian posterior probability and the p value behave, when our theta is known, and we have a single experiment at \\(n=10\\) and \\(m=7\\).\nWe can see that with our fixed hyperparameters for the flat uniform beta, or the U-shaped beta the posterior probability changes We can also see that the frequentist interval tends to be wider than any of the two bayesian credible intervals.\nAs a summary, frequentist intervals have the great property given through all the hard work and math behind their creation, where their coverage is often guaranteed when assumptions are met. Bayesian Intervals, do not have that property but certain prior distribution hyperparameters can induce enough uncertainty to match the coverage of a frequentist interval, but with a reduction in uncertainty around the parameter, as well as having the advantage of exact inference, due to the use of probability distributions.\nNotice how the U-shaped beta prior model peaks closer to the true value for \\(\\theta=0.74\\), while maintaining same coverage and inducing less uncertainty."
  }
]